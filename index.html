<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chenxu Yang (杨晨旭)</title>

  <meta name="author" content="Chenxu Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/ucas.jpg">

  <script type="text/javascript">

    function display(id) {
      var traget = document.getElementById(id);
      if (traget.style.display == "none") {
        traget.style.display = "";
      } else {
        traget.style.display = "none";
      }
    }  
  </script>
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Chenxu Yang (杨晨旭) </name>
                  </p>
                  <p>I am a Ph.D. student at <a href="http://www.iie.ac.cn/">Institute of Information Enginering,</a>  <a
                      href="https://www.ucas.ac.cn/">Chinese Academy of Sciences</a> in Beijing, advised by Prof. <a
                      href="https://people.ucas.edu.cn/~linzheng">Zheng Lin</a>. Previously, I was a research intern in the Pattern Recognition Center (PRC), <a href="https://ai.weixin.qq.com/">WeChat AI</a> at Tencent, supervised by <a href="https://fandongmeng.github.io/">Fandong Meng</a> in 2022. 
                  </p>
                  <p>
                    My research interests include dialogue generation, large language models, and related applications. Please reach out to me via email: <a href="mailto:yangchenxu@iie.ac.cn">yangchenxu@iie.ac.cn</a>.
                  </p>
                  <p style="text-align:center">
                    <a href="images/CV-siqingyi.pdf">CV</a> &nbsp/&nbsp
                    <a href="data/WeChat.png>WeChat</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?hl=en&user=JD8EHtoAAAAJ">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/PhoebusSi/">Github</a> &nbsp/&nbsp
                    <a href="https://huggingface.co/QingyiSi">HuggingFace</a> 
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/qingyisi.png"><img style="width:80%;max-width:80" alt="profile photo"
                      src="images/qingyisi.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <ul>
                    <li> <b>[May 2023]</b> One papers accepted to ACL 2023!</li>
                    <li> <b>[Apr. 2023]</b> I gave a talk on Alpaca-CoT at <a
                        href="https://www.bilibili.com/video/BV1Jg4y1L7Fo/?spm_id_from=333.999.0.0&vd_source=3ff4646975353f1a46760e77349c2818">机器之心</a>.</li>
                    <li> <b>[Mar. 2023]</b> We are excited to release <a
                        href="https://github.com/PhoebusSi/alpaca-CoT/">Alpaca-CoT</a>, an instruction-tuning platform with instruction data collection, parameter-efficient methods and large language models unified interface.
                    <li> <b>[Oct. 2022]</b> Two papers accepted to EMNLP 2022!</li>
                    <li> <b>[Jun. 2022]</b> One paper accepted to MM 2022!</li>   
                    <li> <b>[Nov. 2021]</b> I joined the NLC group at MSRA.</li>
                    <li> <b>[May 2021]</b> One paper accepted to ACL 2021!</li>
                    <li> <b>[May 2021]</b> One paper accepted to IJCAI 2021!</li>
                       
                                                                       
                                                                      
                                                                                                                            
                                                                       
                    <a onclick="return display('old_news');"> ---- show more ----</a>
                    <div id="old_news" style="display: none;">
                       
                      <li> <b>[Mar. 2021]</b> I joined the PRC, WeChat AI at Tencent, supervised by <a href="https://fandongmeng.github.io/">Fandong Meng</a>.</li>                                     
                      <li> <b>[Sep. 2019]</b> I joined the Institute of Information Engineering, UCAS as a Ph.D. student.</li>
                      <li> <b>[Jun. 2019]</b> I received B.Eng. from BLCU. GPA: 3.86/4.0 </li>
                                                             
                                                             
                      
                      
                    </div>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Selected Publications (<a href="images/CV-siqingyi.pdf">full papers</a>)</heading>
                  <p>
                    <a href="https://scholar.google.com/citations?hl=en&user=JD8EHtoAAAAJ">Google Scholar</a> /
                    <a href="https://www.semanticscholar.org/author/Qingyi-Si/84109730">Semantic Scholar</a> / <a
                      href="https://dblp.org/pid/227/6822.html">DBLP</a> 
                      
                  </p>
                  <p>
                    (*: Equal contribution)
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/ACL2023.png' width="190">
                    </div>
                    <img src='images/ACL2023.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2305.06407">
                    <papertitle>Combo of Thinking and Observing for Outside-Knowledge VQA</papertitle>
                  </a>
                  <br>
                  <strong>Qingyi Si</strong>, Chenyu Mo, Zheng Lin, Huishan Ji, Weiping Wang
                  <br>
                  <em>ACL</em>, 2023 &nbsp <font color="red"><strong>(Main Conference)</strong></font>
                  <br>
                  <a href="https://arxiv.org/pdf/2305.06407.pdf">pdf</a>
                  /
                  <a href="https://github.com/PhoebusSi/Thinking-while-Observing">code</a>
                  <p></p>
                  <p>
                    This paper proposes a simple and effective method that mimics human behavior, "thinking while observing", i.e., benefiting from the vast knowledge in natural-language space while making the most of the visual features for better image understanding. Our method establishes a new SoTA accuracy with a 6.17% improvement on OK-VQA.
                  </p>
                </td>
              </tr>

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/EMNLP22-1.png' width="190">
                    </div>
                    <img src='images/EMNLP22-1.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://aclanthology.org/2022.findings-emnlp.271/">
                    <papertitle>Language Prior Is Not the Only Shortcut: A Benchmark for Shortcut Learning in VQA</papertitle>
                  </a>
                  <br>
                  <strong>Qingyi Si</strong>, Fandong Meng, Mingyu Zheng, Zheng Lin, Yuanxin Liu, Peng Fu, Yanan Cao, Weiping Wang, Jie Zhou
                  <br>
                  <em>EMNLP</em>, Findings, 2022 &nbsp  <font color="red"><strong>Received <a href="https://github.com/PhoebusSi/PhoebusSi.github.io/blob/master/images/high_praise.jpg">High Praise</a> from <a href="https://scholar.google.com/citations?user=iS_jP_3dpD8J&hl=en">Damien Teney</a></strong></font>
                  </font>
                  <br>
                  <a href="https://aclanthology.org/2022.findings-emnlp.271.pdf">pdf</a>
                  /
                  <a href="https://phoebussi.github.io/VQA-VS-homepage/">homepage</a>
                  /
                  <a href="https://github.com/PhoebusSi/VQA-VS">code</a>
                  /
                  <a href="xxx">peer review</a>
                  <p></p>
                  <p>
                    To solve the single-shortcut problem and three issues in the use of current OOD benchmark VQACP v2, we construct and publish a new OOD benchmark VQA-VS including nine OOD test sets corresponding to varying shortcuts. Compared with
VQA-CP v2, VQA-VS can provide a more reliable and comprehensive testbed for the reasoning ability of debiasing methods. 
                  </p>
                </td>
              </tr>

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/EMNLP22-2.png' width="190">
                    </div>
                    <img src='images/EMNLP22-2.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://aclanthology.org/2022.findings-emnlp.495/">
                    <papertitle>Towards Robust Visual Question Answering: Making the Most of Biased Samples via Contrastive Learning</papertitle>
                  </a>
                  <br>
                  <strong>Qingyi Si</strong>, Yuanxin Liu, Fandong Meng, Zheng Lin, Peng Fu, Yanan Cao, Weiping Wang, Jie Zhou
                  <br>
                  <em>EMNLP</em>, Findings, 2022
                  <br>
                  <a href="https://aclanthology.org/2022.findings-emnlp.495.pdf">pdf</a>
                  /
                  <a href="https://github.com/PhoebusSi/MMBS">code</a>                
                  <p></p>
                  <p>
                    We propose a novel method to ameliorate the ID-OOD trade-off problem faced by most existing debaising methods for VQA models. Instead of undermining the importance of the biased samples, our method makes
the most of them. . It is compatible with multiple backbone models and debiasing methods, and achieves competitive OOD performance while maintaining ID performance.  
                  </p>
                </td>
              </tr>

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/ACL21.png' width="190">
                    </div>
                    <img src='images/ACL21.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://aclanthology.org/2021.acl-long.317/">
                    <papertitle>Check It Again:Progressive Visual Question Answering via Visual Entailment</papertitle>
                  </a>
                  <br>
                  <strong>Qingyi Si</strong>, Zheng Lin, Ming yu Zheng, Peng Fu, Weiping Wang
                  <br>
                  <em>ACL</em>, 2021 &nbsp <font color="red"><strong>(Main Conference)</strong></font>
                  <br>
                  <a href="https://aclanthology.org/2021.acl-long.317.pdf">pdf</a>
                  /
                  <a href="https://github.com/PhoebusSi/SAR">code</a>
                  <p></p>
                  <p>
                    We propose a select-and-rerank (SAR) progressive framework based on Visual Entailment, which can make full use of the interactive information of image, question and candidate answers. In addition, it is a generic framework, which can be easily combined with
the existing VQA models and further boost their OOD robustness. Our method establishes SoTA with an improvement of 7.55% on the previous best.
                  </p>
                </td>
              </tr>

              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/IJCAI2.png' width="190">
                    </div>
                    <img src='images/IJCAI2.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://www.ijcai.org/proceedings/2021/0540">
                    <papertitle>Learning Class-Transductive Intent Representations for Zero-shot Intent Detection</papertitle>
                  </a>
                  <br>
                  <strong>Qingyi Si</strong>, Yuanxin Liu, Peng Fu, Zheng Lin, Jiangnan Li, Weiping Wang
                  <br>
                  <em>IJCAI</em>, 2021 &nbsp <font color="red"><strong>(13% Acceptance Rate)</strong></font>
                  <br>
                  <a href="https://www.ijcai.org/proceedings/2021/0540.pdf">pdf</a>
                   /
                  <a href="https://github.com/PhoebusSi/CTIR">code</a>
                  <p></p>
                  <p>
                    We propose a class-transductive framework, CTIR, to overcome the limitations of existing zero-shot intent detection models: : 1) They are not good at modeling the relationship between seen and unseen intents. 2) They cannot effectively recognize unseen intents under the generalized intent detection (GZSID) setting.
                  </p>
                </td>
              </tr>
          
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/CompressRobust.png' width="190">
                    </div>
                    <img src='images/CompressRobust.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://arxiv.org/abs/2210.14558">
                    <papertitle>Compressing And Debiasing Vision-Language Pre-Trained Models for Visual Question Answering</papertitle>
                  </a>
                  <br>
                  <strong>Qingyi Si</strong>*, Yuanxin Liu*, Zheng Lin, Peng Fu, Weiping Wang
                  <br>
                  <em>preprint</em>, 2023 
                  <br>
                  <a href="https://arxiv.org/pdf/2210.14558.pdf">pdf</a>
                  /
                  <a href="xxxx">code</a>
                  <p></p>
                  <p>
                    This paper investigates whether a VLP can be compressed and debiased simultaneously by searching sparse and robust subnetworks. To this end, we systematically study the design of a training and compression pipeline to search the subnetworks, as well as the assignment of sparsity to different modality-specific modules. 
                    Our results show that there indeed exist sparse and robust subnetworks, which clearly outperform the debiasing SoTAs with fewer parameters
                  </p>
                </td>
              </tr>
                                                                                                                           
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/MM22.png' width="190">
                    </div>
                    <img src='images/MM22.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548170">
                    <papertitle>Visual Dialog for Spotting the Differences between Pairs of Similar Images</papertitle>
                  </a>
                  <br>
                 Duo Zheng, Fandong Meng, <strong>Qingyi Si</strong>, Hairun Fan, Zipeng Xu, Jie Zhou, Fangxiang Feng, Xiaojie Wang

 
                  <br>
                  <em>MM</em>, 2022 &nbsp
                  <br>
                  <a href="https://dl.acm.org/doi/pdf/10.1145/3503161.3548170">pdf</a>
                  /
                  <a href="https://github.com/zd11024/Spot_Difference">code</a>
                  <p></p>
                  <p>
                    We propose a cooperative object-referring game Dial-the-Diff, where the goal is to locate the different object between two similar images via conversing between questioner and answerer. The task addresses two new challenges in visual dialog, including the difference-oriented dialog strategy and the ability of categorization. 
                  </p>
                </td>
              </tr>

            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Projects & Resources</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/alpaca-cot.png' width="190">
                    </div>
                    <img src='images/alpaca-cot.png' width="190">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <a href="https://github.com/PhoebusSi/alpaca-CoT/">
                    <papertitle>Alpaca-CoT: An Instruction Fine-Tuning Platform with Instruction Data Collection and Unified Large Language Models Interface</papertitle>
                  </a>
                  <br>
                  project led by <strong>Qingyi Si</strong>
                  <p>
                    We unified the interfaces of instruction-tuning data (e.g., CoT data), multiple LLMs and parameter-efficient methods (e.g., lora, p-tuning) together for easy use. Meanwhile, we created a new branch to build a Tabular LLM.
                                                    This project can be found <a href="https://github.com/PhoebusSi/alpaca-CoT/">here</a>. The data collection can be found <a href="https://huggingface.co/datasets/QingyiSi/Alpaca-CoT">here</a>.  
                  </p>
                </td>
              </tr>

            

              
          

            </tbody>
          </table>
          
          
















          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Honors and Awards</heading>
      
                    <li> National Scholarship for Doctoral students (Top 2%, RMB ¥ 30,000).
Ministry of Education of P.R. China. 2021. </li>
                    <li> Merit Student, University of Chinese Academy of Sciences (UCAS). 2020, 2021. </li>
                    <li> Excellent Student Cadre, University of Chinese Academy of Sciences (UCAS). 2020. </li>
                    <li> Excellent Undergraduate Graduation Project (Thesis), Beijing.
Education Commission of Beijing. 2019. </li>
                    <li> Regular Institutions of Higher Education Outstanding Graduate, Beijing.
Education Commission of Beijing. 2019. </li>
                    <li> National Scholarship for Undergraduates (Top 2%, RMB ¥ 8,000).
Ministry of Education of P.R. China. 2017, 2018.</li>
                 
                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=Z0oDxeOzSgyGGvjTzKlY4dUL7ovedE-P_oqVYZVI64g&cl=ffffff&w=a"></script>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's
                    website</a>
                </p>
              </td>
            </tr>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
